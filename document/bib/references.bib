@article{commScope,
   abstract = {A holistic view of the data center and the opportunities to enhance its infrastructure to meet current and future demands},
   author = {CommScope},
   keywords = {CO-110101.3-EN,data center architectures,data center equipment connection,data center topologies,mesh network,mesh point of delivery},
   title = {Chapter 3: Data Center topologies and architectures},
   url = {www.commscope.com},
}
@article{Taal2014,
   abstract = {Storing data in the cloud is becoming a common trend, for both end-customers and data center operators. We propose a method for deciding where to host data storage tasks under the constraint of minimal greenhouse gas emission. The decision on whether to store data locally or store it remotely at a cleaner data center relies on the models for the local and remote data centers and the network connecting them. We conclude that the transport network that connects a local node and a 'cleaner' remote data center plays a significant role in the decision of where to store data, and that the frequency of access of the data is an important and related factor. © 2014 IEEE.},
   author = {Arie Taal and Dexter Drupsteen and Marc X. Makkes and Paola Grosso},
   doi = {10.1109/CCNC.2014.6866547},
   isbn = {9781479923557},
   journal = {2014 IEEE 11th Consumer Communications and Networking Conference, CCNC 2014},
   pages = {50-55},
   publisher = {IEEE Computer Society},
   title = {Storage to energy: Modeling the carbon emission of storage task offloading between data centers},
   year = {2014},
}
@inproceedings{Li2014,
   abstract = {The performance requirements and amount of work of an I/O workload affect the number of storage devices and the run time needed by the workload, and should be included in the calculation of the cost or energy consumption of storage devices. This paper introduces models to calculate the cost and energy consumption of storage devices for running a variety of workloads, categorized by their dominant requirements. Measurements of two latest hard disk and solid-state drive (SSD) are included to illustrate the models in practice. Contrary to common belief, SSD is not the energy efficient choice for many workloads.},
   author = {Yan Li and Darrell D.E. Long},
   doi = {10.1109/MASCOTS.2014.20},
   isbn = {978-1-4799-5610-4},
   journal = {2014 IEEE 22nd International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems},
   keywords = {computer engineering,computer science,data storage systems,energy measurement,measurement,measurement techniques,storage system},
   month = {9},
   pages = {100-105},
   publisher = {IEEE},
   title = {Which Storage Device Is the Greenest? Modeling the Energy Cost of {I/O} Workloads},
   url = {http://ieeexplore.ieee.org/document/7033642/},
   year = {2014},
}
@article{Shehabi2016,
   abstract = {This report estimates historical data center electricity consumption back to 2000, relying on previous studies and historical shipment data, and forecasts consumption out to 2020 based on new trends and the most recent data available. Figure ES-1 provides an estimate of total U.S. data center electricity use (servers, storage, network equipment, and infrastructure) from 2000-2020. In 2014, data centers in the U.S. consumed an estimated 70 billion kWh, representing about 1.8\% of total U.S. electricity consumption. Current study results show data center electricity consumption increased by about 4\% from 2010-2014, a large shift from the 24\% percent increase estimated from 2005-2010 and the nearly 90\% increase estimated from 2000-2005. Energy use is expected to continue slightly increasing in the near future, increasing 4\% from 2014-2020, the same rate as the past five years. Based on current trend estimates, U.S. data centers are projected to consume approximately 73 billion kWh in 2020.},
   author = {Arman Shehabi and Sarah Smith and Dale Sartor and Richard Brown and Magnus Herrlin and Jonathan Koomey and Eric Masanet and Nathaniel Horner and Inês Azevedo and William Lintner},
   city = {Berkeley, CA (United States)},
   doi = {10.2172/1372902},
   institution = {Lawrence Berkeley National Laboratory (LBNL)},
   keywords = {and utilization,consumption,energy conservation},
   month = {6},
   title = {{United States} Data Center Energy Usage Report},
   url = {http://www.osti.gov/servlets/purl/1372902/},
   year = {2016},
}
@misc{Bizo2022,
   author = {Daniel Bizo},
   keywords = {Uptime Institute,Uptime Institute Intelligence,cloud usage,data center PUE,data center benchmarking,data center outage,data center rack density,data center resiliency,data center sector,data center staffing,data center survey,data center sustainability},
   note = {Datacenters are making efforts to improve PUE<br/><br/>Density of racks is increasing<br/>Average rack output is 4-6 kW},
   title = {Uptime Institute Global Data Center Survey 2022 Resiliency remains critical in a volatile world},
   year = {2022},
}
@misc{Storage101,
   title = {Storage 101: Data Center Storage Configurations - Simple Talk},
   url = {https://www.red-gate.com/simple-talk/databases/sql-server/database-administration-sql-server/storage-101-data-center-storage-configurations/},
}
@article{UK-parliament,
   abstract = {This POSTnote gives an overview of the energy use of Information and Communication Technology (ICT), including data centres, communication networks and user devices. It looks at forecasts for future ICT energy use, and how these may be impacted by trends in energy efficiency and emerging applications of ICT. It discusses monitoring and regulation of ICT energy use and associated challenges.},
   title = {Energy Consumption of {ICT}},
   url = {https://researchbriefings.files.parliament.uk/documents/POST-PN-0677/POST-PN-0677.pdf},
   year = {2022},
}

@article{Coroama2014,
   abstract = {Assessing the average energy intensity of Internet transmissions is a complex task that has been a controversial subject of discussion. Estimates published over the last decade diverge by up to four orders of magnitude - from 0.0064. kilowatt-hours per gigabyte (kWh/GB) to 136. kWh/GB. This article presents a review of the methodological approaches used so far in such assessments: i) top-down analyses based on estimates of the overall Internet energy consumption and the overall Internet traffic, whereby average energy intensity is calculated by dividing energy by traffic for a given period of time, ii) model-based approaches that model all components needed to sustain an amount of Internet traffic, and iii) bottom-up approaches based on case studies and generalization of the results. Our analysis of the existing studies shows that the large spread of results is mainly caused by two factors: a) the year of reference of the analysis, which has significant influence due to efficiency gains in electronic equipment, and b) whether end devices such as personal computers or servers are included within the system boundary or not. For an overall assessment of the energy needed to perform a specific task involving the Internet, it is necessary to account for the types of end devices needed for the task, while the energy needed for data transmission can be added based on a generic estimate of Internet energy intensity for a given year. Separating the Internet as a data transmission system from the end devices leads to more accurate models and to results that are more informative for decision makers, because end devices and the networking equipment of the Internet usually belong to different spheres of control. © 2013 Elsevier Inc.},
   author = {Vlad C. Coroama and Lorenz M. Hilty},
   doi = {10.1016/j.eiar.2013.12.004},
   issn = {01959255},
   journal = {Environmental Impact Assessment Review},
   keywords = {Data transmission,Dematerialization,Energy efficiency,Environmental impact,Industrial ecology,Information and communication technologies (ICT)},
   month = {2},
   pages = {63-68},
   publisher = {Elsevier},
   title = {Assessing Internet energy intensity: A review of methods and results},
   volume = {45},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0195925513001121},
   year = {2014},
}
@article{Vereecken2011,
   abstract = {One of the main challenges for the future of information and communication technologies is reduction of the power consumption in telecommunication networks. The key consumers are the home gateways at the customer premises for fixed line access technologies and the base stations for wireless access technologies. However, with increasing bit rates, the share of the core networks could become significant as well. In this article we characterize the power consumption in the different types of networks and discuss strategies to reduce the power consumption. © 2011 IEEE.},
   author = {Willem Vereecken and Ward Van Heddeghem and Margot Deruyck and Bart Puype and Bart Lannoo and Wout Joseph and Didier Colle and Luc Martens and Piet Demeester},
   doi = {10.1109/MCOM.2011.5783986},
   issn = {01636804},
   issue = {6},
   journal = {IEEE Communications Magazine},
   month = {6},
   pages = {62-69},
   title = {Power consumption in telecommunication networks: Overview and reduction strategies},
   volume = {49},
   url = {https://www.researchgate.net/publication/228774201_Power_Consumption_in_Telecommunication_Networks_Overview_and_Reduction_Strategies},
   year = {2011},
}
@misc{forum:huawei,
   title = {Introduction to Access Technologies},
   url = {https://forum.huawei.com/enterprise/en/Introduction-to-Access-Technologies/thread/667263493830754304-667213871523442688},
}
@misc{forum:imda,
   title = {Local Multipoint Distribution Services ({LMDS}) - Infocomm Media Development Authority},
   url = {https://www.imda.gov.sg/regulations-and-licensing-listing/spectrum-management-and-coordination/spectrum-rights-auctions-and-assignment/lmds-spectrum-rights/local-multipoint-distribution-services-lmds},
}
@misc{forum:ctrfantennasinc,
   title = {What Is {WMAN}? - C\&T RF Antennas Manufacturer},
   url = {https://ctrfantennasinc.com/what-is-wman/},
}
@misc{ANSI/TIA-942,
   title = {{TIA}'s {ANSI/TIA-942} Standard - {TIA} Online},
   url = {https://tiaonline.org/products-and-services/tia942certification/ansi-tia-942-standard/},
}

@software{bzip2,
  author = {Julian Seward, Federico Mena Quintero and  Micah Snyder},
  title = {{Bzip2}},
  url = {https://gitlab.com/bzip2/bzip2/},
  version = {1.0},
  year = {2022}
}

@misc{cmix,
   title = {Byron Knoll - Cmix},
   url = {https://www.byronknoll.com/cmix.html},
}

@article{Mahoney2002,
   abstract = {This paper describes the PAQ1 lossless data compression program. PAQ1 is an arithmetic encoder using a weighted average of five bit-level predictors. The five models are: (1) a bland model with 0 or 1 equally likely, (2) a set of order-1 through 8 nonstationary n-gram models, (3) a string matching model for n-grams longer than 8, (4) a nonstationary word unigram and bigram model for English text, and (5) a positional context model for data with fixed length records. Probabilities are weighted roughly by n 2 /tp(0)p(1) where n is the context length, t is the age of the training data (number of subsequent events), and p(0) and p(1) are the probabilities of a 0 or 1 (favoring long runs of zeros or ones). The aging of training statistics makes the model nonstationary, which gives excellent compression for mixed data types. PAQ1 compresses the concatenated Calgary corpus to 1.824 bits per character, which is 4.5\% better than RK (Taylor, 1999) and 2.9\% better than PPMONSTR (Shkarin, 2001), the top programs rated by Gilchrist (2001) and Ratushnyak (2001) respectively, although those programs do slightly better on homogeneous data.},
   author = {Matthew V Mahoney},
   isbn = {4025.48125.1},
   title = {The {PAQ1} Data Compression Program},
   year = {2002},
}

@software{jarvis3,
  author = {Diogo Pratas},
  title = {{Jarvis3}},
  url = {https://github.com/cobilab/jarvis3},
}

@software{fqzcomp,
  author = {James Bonfield},
  title = {{fqzcomp}},
  url = {https://github.com/jkbonfield/fqzcomp},
  version = {4.6},
}

@article{Benoit2015,
   abstract = {Background: Data volumes generated by next-generation sequencing (NGS) technologies is now a major concern for both data storage and transmission. This triggered the need for more efficient methods than general purpose compression tools, such as the widely used gzip method. Results: We present a novel reference-free method meant to compress data issued from high throughput sequencing technologies. Our approach, implemented in the software Leon, employs techniques derived from existing assembly principles. The method is based on a reference probabilistic de Bruijn Graph, built de novo from the set of reads and stored in a Bloom filter. Each read is encoded as a path in this graph, by memorizing an anchoring kmer and a list of bifurcations. The same probabilistic de Bruijn Graph is used to perform a lossy transformation of the quality scores, which allows to obtain higher compression rates without losing pertinent information for downstream analyses. Conclusions: Leon was run on various real sequencing datasets (whole genome, exome, RNA-seq or metagenomics). In all cases, LEON showed higher overall compression ratios than state-of-the-art compression software. On a C. elegans whole genome sequencing dataset, LEON divided the original file size by more than 20. Leon is an open source software, distributed under GNU affero GPL License, available for download at http://gatb.inria.fr/software/leon/.},
   author = {Gaëtan Benoit and Claire Lemaitre and Dominique Lavenier and Erwan Drezen and Thibault Dayris and Raluca Uricaru and Guillaume Rizk},
   doi = {10.1186/S12859-015-0709-7},
   issn = {14712105},
   issue = {1},
   journal = {BMC Bioinformatics},
   keywords = {Bloom filter,Compression,NGS,de Bruijn Graph},
   month = {9},
   pages = {288},
   pmid = {26370285},
   publisher = {BMC},
   title = {Reference-free compression of high throughput sequencing data with a probabilistic de Bruijn graph},
   volume = {16},
   url = {/pmc/articles/PMC4570262/ /pmc/articles/PMC4570262/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4570262/},
   year = {2015},
}
@software{NNCP,
   author = {Fabrice Bellard},
   title = {{NNCP}: Lossless Data Compression with Neural Networks},
   url = {https://bellard.org/nncp/},
}
@article{Pinho2014,
   abstract = {Motivation: The data deluge phenomenon is becoming a serious problem in most genomic centers. To alleviate it, general purpose tools, such as gzip, are used to compress the data. However, although pervasive and easy to use, these tools fall short when the intention is to reduce as much as possible the data, for example, for medium-and long-term storage. A number of algorithms have been proposed for the compression of genomics data, but unfortunately only a few of them have been made available as usable and reliable compression tools.Results: In this article, we describe one such tool, MFCompress, specially designed for the compression of FASTA and multi-FASTA files. In comparison to gzip and applied to multi-FASTA files, MFCompress can provide additional average compression gains of almost 50\%, i.e. it potentially doubles the available storage, although at the cost of some more computation time. On highly redundant datasets, and in comparison with gzip, 8-fold size reductions have been obtained.Availability: Both source code and binaries for several operating systems are freely available for non-commercial use at http://bioinformatics.ua. pt/software/mfcompress/.Contact: Supplementary information: Supplementary data are available at Bioinformatics online. © 2013 The Author .},
   author = {Armando J. Pinho and Diogo Pratas},
   doi = {10.1093/BIOINFORMATICS/BTT594},
   issn = {1367-4803},
   issue = {1},
   journal = {Bioinformatics},
   month = {1},
   pages = {117-118},
   pmid = {24132931},
   publisher = {Oxford Academic},
   title = {MFCompress: a compression tool for {FASTA and multi-FASTA data}},
   volume = {30},
   url = {https://dx.doi.org/10.1093/bioinformatics/btt594},
   year = {2014},
}
@article{Kryukov2019,
   abstract = {Summary: DNA sequence databases use compression such as gzip to reduce the required storage space and network transmission time. We describe Nucleotide Archival Format (NAF) - a new file format for lossless reference-free compression of FASTA and FASTQ-formatted nucleotide sequences. Nucleotide Archival Format compression ratio is comparable to the best DNA compressors, while providing dramatically faster decompression. We compared our format with DNA compressors: DELIMINATE and MFCompress, and with general purpose compressors: gzip, bzip2, xz, brotli and zstd. Availability and implementation: NAF compressor and decompressor, as well as format specification are available at https://github.com/KirillKryukov/naf. Format specification is in public domain. Compressor and decompressor are open source under the zlib/libpng license, free for nearly any use. Supplementary information: Supplementary data are available at Bioinformatics online.},
   author = {Kirill Kryukov and Mahoko Takahashi Ueda and So Nakagawa and Tadashi Imanishi},
   doi = {10.1093/BIOINFORMATICS/BTZ144},
   issn = {1367-4803},
   issue = {19},
   journal = {Bioinformatics},
   month = {10},
   pages = {3826-3828},
   pmid = {30799504},
   publisher = {Oxford Academic},
   title = {Nucleotide Archival Format ({NAF}) enables efficient lossless reference-free compression of {DNA} sequences},
   volume = {35},
   url = {https://dx.doi.org/10.1093/bioinformatics/btz144},
   year = {2019},
}
@article{Deorowicz2023,
   abstract = {Motivation: High-quality sequence assembly is the ultimate representation of complete genetic information of an individual. Several ongoing pangenome projects are producing collections of high-quality assemblies of various species. Each project has already generated assemblies of hundreds of gigabytes on disk, greatly impeding the distribution of and access to such rich datasets. Results: Here, we show how to reduce the size of the sequenced genomes by 2-3 orders of magnitude. Our tool compresses the genomes significantly better than the existing programs and is much faster. Moreover, its unique feature is the ability to access any contig (or its part) in a fraction of a second and easily append new samples to the compressed collections. Thanks to this, AGC could be useful not only for backup or transfer purposes but also for routine analysis of pangenome sequences in common pipelines. With the rapidly reduced cost and improved accuracy of sequencing technologies, we anticipate more comprehensive pangenome projects with much larger sample sizes. AGC is likely to become a foundation tool to store, distribute and access pangenome data.},
   author = {Sebastian Deorowicz and Agnieszka Danek and Heng Li},
   doi = {10.1093/BIOINFORMATICS/BTAD097},
   issn = {13674811},
   issue = {3},
   journal = {Bioinformatics},
   month = {3},
   publisher = {Oxford Academic},
   title = {{AGC}: compact representation of assembled genomes with fast queries and updates},
   volume = {39},
   url = {https://dx.doi.org/10.1093/bioinformatics/btad097},
   year = {2023},
}
@article{Jones2012,
   abstract = {We present Quip, a lossless compression algorithm for next-generation sequencing data in the FASTQ and SAM/BAM formats. In addition to implementing reference-based compression, we have developed, to our knowledge, the first assembly-based compressor, using a novel de novo assembly algorithm. A probabilistic data structure is used to dramatically reduce the memory required by traditional de Bruijn graph assemblers, allowing millions of reads to be assembled very efficiently. Read sequences are then stored as positions within the assembled contigs. This is combined with statistical compression of read identifiers, quality scores, alignment information and sequences, effectively collapsing very large data sets to <15\% of their original size with no loss of information. Availability: Quip is freely available under the 3-clause BSD license from http://cs.washington.edu/homes/dcjones/quip. © 2012 The Author(s).},
   author = {Daniel C. Jones and Walter L. Ruzzo and Xinxia Peng and Michael G. Katze},
   doi = {10.1093/NAR/GKS754},
   issn = {03051048},
   issue = {22},
   journal = {Nucleic Acids Research},
   month = {12},
   pages = {e171},
   pmid = {22904078},
   publisher = {Oxford University Press},
   title = {Compression of next-generation sequencing reads aided by highly efficient de novo assembly},
   volume = {40},
   url = {/pmc/articles/PMC3526293/ /pmc/articles/PMC3526293/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3526293/},
   year = {2012},
}
@misc{lzma,
   title = {{LZMA SDK (Software Development Kit)}},
   url = {https://7-zip.org/sdk.html},
}
@article{Cheung2018,
   abstract = {Due to the rapid rise of power consumption of data centers in recent years, much work has been done to develop energy-efficient design, controls and diagnosis of their cooling systems, while the energy system simulation is used as an effective tool. However, existing models of information technology (IT) equipment of data centers cannot well represent the effects of IT equipment design and operation status on the data center cooling demand, and this hinders the development of the energy saving cooling technologies of data centers. To address this issue, this paper introduces a power consumption model of IT equipment in data centers with coefficients and modeling script provided for immediate use in data center energy system simulation. This energy model can be used to simulate energy performance of typical IT equipment in data centers under real-time dynamic operation conditions conveniently and effectively without the need of data other than the specifications of a data center design and IT equipment manuals. Its use with a commonly used building simulation program is demonstrated with a building model of a typical large office in a subtropical area. The results show that the model can represent the change of power consumption of data centers with different IT equipment designs and operation appropriately.},
   author = {Howard Cheung and Shengwei Wang and Chaoqun Zhuang and Jiefan Gu},
   doi = {10.1016/J.APENERGY.2018.03.138},
   issn = {0306-2619},
   journal = {Applied Energy},
   keywords = {Building simulation,Data center,Energy consumption modeling,Energy efficiency},
   month = {7},
   pages = {329-342},
   publisher = {Elsevier},
   title = {A simplified power consumption model of information technology ({IT}) equipment in data centers for energy system real-time dynamic simulation},
   volume = {222},
   year = {2018},
}
@article{Dayarathna2016,
   abstract = {Data centers are critical, energy-hungry infrastructures that run large-scale Internet-based services. Energy consumption models are pivotal in designing and optimizing energy-efficient operations to curb excessive energy consumption in data centers. In this paper, we survey the state-of-the-art techniques used for energy consumption modeling and prediction for data centers and their components. We conduct an in-depth study of the existing literature on data center power modeling, covering more than 200 models. We organize these models in a hierarchical structure with two main branches focusing on hardware-centric and software-centric power models. Under hardware-centric approaches we start from the digital circuit level and move on to describe higher-level energy consumption models at the hardware component level, server level, data center level, and finally systems of systems level. Under the software-centric approaches we investigate power models developed for operating systems, virtual machines and software applications. This systematic approach allows us to identify multiple issues prevalent in power modeling of different levels of data center systems, including: i) few modeling efforts targeted at power consumption of the entire data center ii) many state-of-the-art power models are based on a few CPU or server metrics, and iii) the effectiveness and accuracy of these power models remain open questions. Based on these observations, we conclude the survey by describing key challenges for future research on constructing effective and accurate data center power models.},
   author = {Miyuru Dayarathna and Yonggang Wen and Rui Fan},
   doi = {10.1109/COMST.2015.2481183},
   issn = {1553877X},
   issue = {1},
   journal = {IEEE Communications Surveys and Tutorials},
   keywords = {Cloud Computing,Data Center,Energy Consumption Modeling,Energy Efficiency},
   month = {1},
   pages = {732-794},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Data center energy consumption modeling: A survey},
   volume = {18},
   year = {2016},
}

@article{Incorporated2007,
   abstract = {This report was prepared in response to the request from Congress stated in Public Law 109-431 (H.R. 5646),"An Act to Study and Promote the Use of Energy Efficient Computer Servers in the United States." This report assesses current trends in energy use and energy costs of data centers and servers in the U.S. (especially Federal government facilities) and outlines existing and emerging opportunities for improved energy efficiency. It also makes recommendations for pursuing these energy-efficiency opportunities broadly across the country through the use of information and incentive-based programs.},
   author = {ICF Incorporated and ERG Incorporated and Richard E Brown and Richard Brown and Eric Masanet and Bruce Nordman and Bill Tschudi and Arman Shehabi and John Stanley and Jonathan Koomey and Dale Sartor and Peter Chan and Joe Loper and Steve Capana and Bruce Hedman and Rebecca Duff and Evan Haines and Danielle Sass and Andrew Fanara},
   city = {Berkeley, CA},
   doi = {10.2172/929723},
   institution = {Lawrence Berkeley National Laboratory (LBNL)},
   keywords = {computers,energy accounting,energy efficiency,national government,public law,recommendations},
   month = {8},
   title = {Report to Congress on Server and Data Center Energy Efficiency: Public Law 109-431},
   url = {http://www.osti.gov/servlets/purl/929723-4d6s1A/},
   year = {2007},
}
@article{Schien2013,
   abstract = {In this study, we use an improved, more accurate model to analyze the energy footprint of content downloaded from a major online newspaper by means of various combinations of user devices and access networks. Our results indicate that previous analyses based on average figures for laptops or desktop personal computers predict national and global energy consumption values that are unrealistically high. Additionally, we identify the components that contribute most of the total energy consumption during the use stage of the life cycle of digital services. We find that, depending on the type of user device and access network employed, the data center where the news content originates consumes between 4\% and 48\% of the total energy consumption when news articles are read and between 2\% and 11\% when video content is viewed. Similarly, we find that user devices consume between 7\% and 90\% and 0.7\% and 78\% for articles and video content, respectively, depending on the type of user device and access network that is employed. Though increasing awareness of the energy consumption by data centers is justified, an analysis of our results shows that for individual users of the online newspaper we studied, energy use by user devices and the third-generation (3G) mobile network are usually bigger contributors to the service footprint than the datacenters. Analysis of our results also shows that data transfer of video content has a significant energy use on the 3G mobile network, but less so elsewhere. Hence, a strategy of reducing the resolution of video would reduce the energy footprint for individual users who are using mobile devices to access content by the 3G network. © 2013 by Yale University.},
   author = {Daniel Schien and Paul Shabajee and Mike Yearworth and Chris Preist},
   doi = {10.1111/JIEC.12065},
   issn = {1530-9290},
   issue = {6},
   journal = {Journal of Industrial Ecology},
   keywords = {Internet,Monte Carlo simulation,communications technology (ICT),industrial ecology,information and,life cycle assessment (LCA),online publishing},
   month = {12},
   pages = {800-813},
   publisher = {John Wiley & Sons, Ltd},
   title = {Modeling and Assessing Variability in Energy Consumption During the Use Stage of Online Multimedia Services},
   volume = {17},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1111/jiec.12065 https://onlinelibrary.wiley.com/doi/abs/10.1111/jiec.12065 https://onlinelibrary.wiley.com/doi/10.1111/jiec.12065},
   year = {2013},
}
@article{Ciaglia2007,
   author = {Christian Ciaglia and Miklas Hahn and Eric Labouze and Eva Leonhardt and Linda Lescuyer and Veronique Monier and Shailendra Mudgal and Nils F Nissen and Elodie Pechenart and Marina Proske and Karsten Schischke and Alexander Schlösser and Lutz Stobbe and Lea Turunen},
   title = {{EuP} Preparatory Study Lot 6 "Standby and Off-mode Losses" Final Report Consortium List Individual Authors (in alphabetical order)},
   url = {http://www.izm.fraunhofer.de http://www.biois.com http://www.duh.de},
}
@misc{IEA,
   title = {Data centres \& networks - {IEA}},
   url = {https://www.iea.org/energy-system/buildings/data-centres-and-data-transmission-networks},
}
@misc{GeekyHumans,
   title = {15 Most Popular Data Compression Algorithms - Geeky Humans},
   url = {https://geekyhumans.com/de/most-popular-data-compression-algorithms/},
}



